{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b782f06",
   "metadata": {},
   "source": [
    "# Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d293145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path so we can import our modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "from knowledge_base.src.ingestion.sec_downloader import SECDownloader\n",
    "\n",
    "# Create our downloader instance\n",
    "downloader = SECDownloader()\n",
    "\n",
    "# Test the connection by getting basic company info\n",
    "company_info = downloader.get_company_info('MSTR')\n",
    "print(\"Company Info for MSTR:\")\n",
    "print(company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62181a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Download and process filings\n",
    "filings = downloader.download_company_filings(\n",
    "    ticker='RDDT',\n",
    "    filing_types=['10-K'],  # Just annual reports\n",
    "    num_filings=2  # Get the most recent filings\n",
    ")\n",
    "\n",
    "print(\"\\nDownloaded Filing Metadata:\")\n",
    "for filing in filings:\n",
    "    print(f\"\\nFiling Type: {filing.get('type')}\")\n",
    "    print(f\"Filing Date: {filing.get('period_of_report', 'N/A')}\")\n",
    "    print(f\"Accession Number: {filing.get('accession_number', 'N/A')}\")\n",
    "    print(f\"File Path: {filing.get('file_path')}\")\n",
    "    \n",
    "    # Get metadata file path\n",
    "    doc_dir = Path(filing['file_path']).parent\n",
    "    metadata_path = doc_dir / \"metadata.json\"\n",
    "    \n",
    "    # Read and display the saved metadata\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            print(\"\\nStored Metadata:\")\n",
    "            print(json.dumps(metadata, indent=2))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "117b31fd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Enhanced Metadata Storage for Federated Knowledge Base\n",
    "\n",
    "Each SEC filing is now stored with comprehensive metadata to support integration with multiple knowledge stores:\n",
    "\n",
    "### Storage Location\n",
    "- Metadata is stored as `metadata.json` in the same directory as its raw document\n",
    "- Example path: `data/raw/sec-edgar-filings/AAPL/10-K/0000320193-22-000108/metadata.json`\n",
    "\n",
    "### Metadata Schema\n",
    "\n",
    "1. **Document Identification**\n",
    "   - Unique document ID (SEC accession number)\n",
    "   - Ticker symbol\n",
    "   - Filing type\n",
    "   - Filing date\n",
    "   - Accession number\n",
    "\n",
    "2. **File Information**\n",
    "   - Path to raw document\n",
    "   - Filename\n",
    "   - File size\n",
    "   - Last modified timestamp\n",
    "\n",
    "3. **Company Information**\n",
    "   - Company name\n",
    "   - CIK (Central Index Key)\n",
    "   - Fiscal year end\n",
    "   - Business address\n",
    "\n",
    "4. **Document Metadata**\n",
    "   - Title\n",
    "   - Source (SEC EDGAR)\n",
    "   - Language\n",
    "   - Document type\n",
    "   - Filing period\n",
    "   - Document date\n",
    "\n",
    "5. **Processing Metadata**\n",
    "   - Download timestamp\n",
    "   - Download agent\n",
    "   - Source API\n",
    "   - Schema version\n",
    "\n",
    "6. **Knowledge Base Integration**\n",
    "   - Vector store ID (for semantic search)\n",
    "   - SQL record ID (for structured data)\n",
    "   - Graph node ID (for relationship mapping)\n",
    "   - Related documents\n",
    "   - Searchable tags\n",
    "\n",
    "This enhanced metadata structure supports:\n",
    "- Cross-referencing between different storage systems\n",
    "- Rich document relationships and context\n",
    "- Efficient document retrieval and filtering\n",
    "- Integration with SQL, vector, and graph databases\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Metadata Storage\n",
    "\n",
    "The SEC filing metadata is now stored in JSON files in the `data/raw/metadata` directory. Each metadata file contains:\n",
    "\n",
    "1. Basic filing information:\n",
    "   - Ticker symbol\n",
    "   - Filing type (10-K, 10-Q, etc.)\n",
    "   - Filing date\n",
    "   - File path to the raw document\n",
    "   - Accession number\n",
    "   - File size\n",
    "   - Download timestamp\n",
    "\n",
    "2. Extracted document metadata:\n",
    "   - Company name\n",
    "   - CIK (Central Index Key)\n",
    "   - Fiscal year end\n",
    "   - Period of report\n",
    "   - Business address\n",
    "   - Financial indicators (presence of revenue data, profit data, etc.)\n",
    "\n",
    "The metadata files are named using the pattern: `{ticker}_{type}_{date}_metadata.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce4585",
   "metadata": {},
   "source": [
    "# Process Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7054a2",
   "metadata": {},
   "source": [
    "## Processing for SQL Database data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_TICKER = 'RDDT'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from knowledge_base.src.ingestion.sec_downloader import SECDownloader\n",
    "from knowledge_base.src.ingestion.sec_sql_extractor import SECDataExtractor\n",
    "from knowledge_base.src.storage.sql_manager import FinancialMetricsManager\n",
    "\n",
    "\n",
    "# Force reload the updated module\n",
    "import importlib\n",
    "import knowledge_base.src.ingestion.sql_extractor\n",
    "importlib.reload(knowledge_base.src.ingestion.sec_downloader)\n",
    "importlib.reload(knowledge_base.src.ingestion.sql_extractor)\n",
    "importlib.reload(knowledge_base.src.storage.sql_manager)\n",
    "\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {Path.cwd().parent}\")\n",
    "\n",
    "# Then import the extractor\n",
    "from knowledge_base.src.ingestion.sql_extractor import SECDataExtractor\n",
    "\n",
    "# Initialize components\n",
    "downloader = SECDownloader()\n",
    "extractor = SECDataExtractor()\n",
    "sql_manager = FinancialMetricsManager()\n",
    "\n",
    "# Step 1: Download SEC filing\n",
    "filings = downloader.download_company_filings(\n",
    "    ticker=TEST_TICKER,\n",
    "    filing_types=['10-K'],\n",
    "    num_filings=5\n",
    ")\n",
    "\n",
    "print(\"len(filings)\", len(filings))\n",
    "\n",
    "# Step 2: Process the filing\n",
    "if filings:\n",
    "\n",
    "    print(\"if filings called\")\n",
    "    \n",
    "    filing = filings[0]\n",
    "    print(\"filing\", filing)\n",
    "    file_dir = Path(filing['file_path']).parent\n",
    "    \n",
    "    # Look for XBRL or HTML version\n",
    "    xbrl_file = next(file_dir.glob(\"*.xml\"), None)\n",
    "    html_file = next(file_dir.glob(\"*.htm*\"), None)\n",
    "\n",
    "\n",
    "    # Add client to the database before adding the document\n",
    "    client_data = {\n",
    "        \"id\": filing['ticker'],\n",
    "        \"company_name\": filing.get('company_name', ''),\n",
    "        \"cik\": filing.get('cik', ''),\n",
    "        \"industry\": \"\",  # Fill if available\n",
    "        \"sector\": \"\",    # Fill if available\n",
    "        \"market_cap\": None  # Fill if available\n",
    "    }\n",
    "    sql_manager.sql_store.add_client(client_data)\n",
    "    \n",
    "    # Create document in SQL store first\n",
    "    doc_data = {\n",
    "        \"document_id\": filing['accession_number'],\n",
    "        \"client_id\": filing['ticker'],\n",
    "        \"filing_type\": filing['type'],\n",
    "        \"filing_date\": filing['period_of_report'],\n",
    "        \"file_path\": filing['file_path'],\n",
    "        \"file_size\": filing['file_size'],\n",
    "        \"download_date\": datetime.fromisoformat(filing['downloaded_at']),\n",
    "        \"has_revenue_data\": filing['has_revenue_data'],\n",
    "        \"has_profit_data\": filing['has_profit_data'],\n",
    "        \"has_balance_sheet\": filing['has_balance_sheet'],\n",
    "        \"has_cash_flow\": filing['has_cash_flow']\n",
    "    }\n",
    "    \n",
    "    # Add document to get SQL document_id\n",
    "    document_id = sql_manager.sql_store.add_document(doc_data)\n",
    "    \n",
    "    if document_id:\n",
    "\n",
    "        print(\"IF document_id CALLED\")\n",
    "\n",
    "        # Try XBRL first, then HTML, then full submission\n",
    "        if xbrl_file:\n",
    "            print(f\"Processing XBRL file: {xbrl_file}\")\n",
    "            metrics = extractor.process_document(str(xbrl_file), TEST_TICKER)\n",
    "        elif html_file:\n",
    "            print(f\"Processing HTML file: {html_file}\")\n",
    "            metrics = extractor.process_document(str(html_file), TEST_TICKER)\n",
    "        else:\n",
    "            print(f\"Processing full submission: {filing['file_path']}\")\n",
    "            metrics = extractor.process_document(filing['file_path'], TEST_TICKER)\n",
    "            \n",
    "        print(f\"Extracted {len(metrics)} metrics\")\n",
    "        if metrics:\n",
    "            sql_manager.save_extracted_metrics(metrics, document_id)\n",
    "            print(\"Metrics saved to database\")\n",
    "            print(\"metrics\", metrics)\n",
    "\n",
    "        # Validate the extractions\n",
    "        validation = sql_manager.validate_client_metrics(TEST_TICKER, 2024)\n",
    "        print(\"\\nValidation results:\", validation)\n",
    "\n",
    "        # Get comparative metrics\n",
    "        comparative = sql_manager.get_comparative_metrics(\n",
    "            [\"AAPL\", \"MSFT\", \"GOOGL\"],\n",
    "            [\"revenue\", \"net_income\"],\n",
    "            2024\n",
    "        )\n",
    "        print(\"\\nComparative metrics:\", comparative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c8f1a",
   "metadata": {},
   "source": [
    "## Quick test for SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57287db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Simple SQL Database Test\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import necessary modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# # Add the project root to Python path so we can import our modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Force reload the updated module\n",
    "import importlib\n",
    "import knowledge_base.src.ingestion.sql_extractor\n",
    "importlib.reload(knowledge_base.src.ingestion.sec_downloader)\n",
    "importlib.reload(knowledge_base.src.ingestion.sql_extractor)\n",
    "importlib.reload(knowledge_base.src.storage.sql_manager)\n",
    "\n",
    "# # Database path\n",
    "db_path = Path(\"../knowledge_base/data/financial_kb.db\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED DATABASE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Connect to database for detailed analysis\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # Get all tables\n",
    "    tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "    print(f\"Tables found: {tables['name'].tolist()}\")\n",
    "    \n",
    "    # Analyze each table\n",
    "    for table_name in tables['name']:\n",
    "        print(f\"\\n--- {table_name.upper()} TABLE ---\")\n",
    "        \n",
    "        # Get table schema\n",
    "        schema = pd.read_sql_query(f\"PRAGMA table_info({table_name})\", conn)\n",
    "        print(f\"Columns: {schema['name'].tolist()}\")\n",
    "        \n",
    "        # Get record count\n",
    "        count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table_name}\", conn)\n",
    "        record_count = count['count'].iloc[0]\n",
    "        print(f\"Records: {record_count}\")\n",
    "        \n",
    "        # Show sample data if records exist\n",
    "        if record_count > 0:\n",
    "            sample = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 200\", conn)\n",
    "            print(\"Sample data:\")\n",
    "            print(sample.to_string(index=False))\n",
    "        else:\n",
    "            print(\"(No data)\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(f\"\\n=== ADDITIONAL ANALYSIS ===\")\n",
    "    \n",
    "    # Check for any financial metrics\n",
    "    metrics_count = pd.read_sql_query(\"SELECT COUNT(*) as count FROM financial_metrics\", conn)['count'].iloc[0]\n",
    "    print(f\"Financial metrics extracted: {metrics_count}\")\n",
    "    \n",
    "    # Check for any clients\n",
    "    clients_count = pd.read_sql_query(\"SELECT COUNT(*) as count FROM clients\", conn)['count'].iloc[0]\n",
    "    print(f\"Clients registered: {clients_count}\")\n",
    "    \n",
    "    # Check document processing status\n",
    "    docs = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            client_id,\n",
    "            filing_type,\n",
    "            filing_date,\n",
    "            has_revenue_data,\n",
    "            has_profit_data,\n",
    "            has_balance_sheet,\n",
    "            has_cash_flow,\n",
    "            financial_density\n",
    "        FROM documents\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    if not docs.empty:\n",
    "        print(f\"\\nDocument processing status:\")\n",
    "        print(docs.to_string(index=False))\n",
    "    \n",
    "    # Check for any chunks\n",
    "    chunks_count = pd.read_sql_query(\"SELECT COUNT(*) as count FROM document_chunks\", conn)['count'].iloc[0]\n",
    "    print(f\"Document chunks created: {chunks_count}\")\n",
    "    \n",
    "    print(f\"\\n✅ Database analysis complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error analyzing database: {e}\")\n",
    "\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ca72e",
   "metadata": {},
   "source": [
    "## Processing for Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for processing document chunks for Vector DB\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from knowledge_base.src.ingestion.document_processor import FinancialDocumentProcessor\n",
    "from knowledge_base.src.ingestion.sec_downloader import SECDownloader\n",
    "from knowledge_base.config.settings import get_settings\n",
    "\n",
    "# Initialize settings and components\n",
    "settings = get_settings()\n",
    "processor = FinancialDocumentProcessor()\n",
    "downloader = SECDownloader()\n",
    "\n",
    "# Define test parameters\n",
    "TEST_TICKER = \"NVDA\"  # Example: NVIDIA\n",
    "TEST_FILING_TYPES = [\"10-K\"]  # Or include \"8-K\", \"10-Q\"\n",
    "TEST_OUTPUT_DIR = Path(settings.data.processed_data_path)\n",
    "TEST_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Step 1: Download filings\n",
    "downloaded_filings = downloader.download_company_filings(\n",
    "    ticker=TEST_TICKER,\n",
    "    filing_types=TEST_FILING_TYPES,\n",
    "    num_filings=1  # Just get the most recent one\n",
    ")\n",
    "print(f\"\\nDownloaded {len(downloaded_filings)} filings\")\n",
    "\n",
    "# Step 2: Process each filing\n",
    "all_processed_chunks = []\n",
    "for filing in downloaded_filings:\n",
    "    print(f\"\\nProcessing {filing['type']} filing from {filing['file_path']}\")\n",
    "    \n",
    "    # Add processing metadata\n",
    "    filing['source'] = 'SEC Edgar'\n",
    "    filing['processed_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Process the filing\n",
    "    chunks = processor.process_sec_filing(\n",
    "        file_path=filing['file_path'],\n",
    "        metadata=filing\n",
    "    )\n",
    "    all_processed_chunks.extend(chunks)\n",
    "    \n",
    "    # Save processed chunks\n",
    "    output_file = f\"{TEST_TICKER}_{filing['type']}_{filing['date']}_processed.json\"\n",
    "    processor.save_processed_chunks(chunks, output_file)\n",
    "    print(f\"Saved processed chunks to: {output_file}\")\n",
    "\n",
    "# Step 3: Inspect results\n",
    "print(f\"\\nTotal chunks generated: {len(all_processed_chunks)}\")\n",
    "if all_processed_chunks:\n",
    "    print(\"\\nSample chunk content:\")\n",
    "    print(all_processed_chunks[0].page_content[:500])\n",
    "    print(\"\\nSample chunk metadata:\")\n",
    "    print(all_processed_chunks[0].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
